import os
import requests  # ƒë·ªÉ b·∫Øt l·ªói timeout r√µ r√†ng
import streamlit as st
from rag import RAGIndex
from models import LLMProvider
from prompts import SYSTEM_PROMPT
from web_ingest import load_vendor_urls, fetch_vendor_docs

st.set_page_config(page_title="SDN302 NodeJS Course Assistant", page_icon="üì±", layout="wide")

# Sidebar
with st.sidebar:
    st.title("‚öôÔ∏è C·∫•u h√¨nh")

    # M·∫≠t kh·∫©u l·ªõp (n·∫øu APP_PASSWORD c√≥ trong Secrets)
    APP_PASSWORD = os.getenv("APP_PASSWORD")
    pw = None
    if APP_PASSWORD:
        pw = st.text_input("Password", type="password", help="Nh·∫≠p m·∫≠t kh·∫©u l·ªõp ƒë·ªÉ truy c·∫≠p.")

    # Provider
    provider_env = os.getenv("PROVIDER", "github").lower()
    provider = st.selectbox("Provider", ["github", "google"], index=0 if provider_env == "github" else 1)

    # Ch·ªçn model ngay tr√™n UI (t√πy provider)
    gh_default_model = os.getenv("GITHUB_MODELS_MODEL", "gpt-4o-mini")
    gg_default_model = os.getenv("GOOGLE_MODEL", "gemini-1.5-pro")
    if provider == "github":
        model_name = st.text_input("Model (GitHub Models)", gh_default_model, help="V√≠ d·ª•: gpt-4o-mini, gpt-4o")
    else:
        model_name = st.text_input("Model (Gemini)", gg_default_model, help="V√≠ d·ª•: gemini-1.5-pro, gemini-1.5-flash")

    # T√πy ch·ªçn RAG v√† tham s·ªë
    use_rag = st.checkbox("D√πng RAG (tr√≠ch t√†i li·ªáu)", value=True)
    use_local_docs = st.checkbox("D√πng t√†i li·ªáu n·ªôi b·ªô (data/)", value=True)
    use_vendor_docs = st.checkbox("D√πng ngu·ªìn vendor (sources.yaml)", value=True)
    top_k = st.slider("S·ªë ƒëo·∫°n tr√≠ch d·∫´n (k)", 1, 8, 4)
    temperature = st.slider("Nhi·ªát ƒë·ªô (creativity)", 0.0, 1.0, 0.3)

    # N√∫t test + placeholder hi·ªÉn th·ªã k·∫øt qu·∫£
    test_clicked = st.button("üß™ Test k·∫øt n·ªëi LLM")
    ping_placeholder = st.empty()

    st.markdown("---")
    st.caption("Qu·∫£n l√Ω API keys trong Streamlit Secrets. Kh√¥ng commit secrets l√™n GitHub.")

# Ch·∫∑n truy c·∫≠p n·∫øu c√≥ m·∫≠t kh·∫©u nh∆∞ng ch∆∞a nh·∫≠p ƒë√∫ng
if APP_PASSWORD and (pw or "") != APP_PASSWORD:
    st.info("·ª®ng d·ª•ng y√™u c·∫ßu m·∫≠t kh·∫©u l·ªõp. Vui l√≤ng nh·∫≠p ·ªü sidebar.")
    st.stop()

# Cache resources
@st.cache_resource(show_spinner=True)
def load_index():
    idx = RAGIndex(data_dir="data")
    idx.build()  # ch·ªâ n·ªôi b·ªô tr∆∞·ªõc
    return idx

@st.cache_resource(show_spinner=True)
def load_llm(provider_choice: str, ui_model: str):
    # √Åp d·ª•ng l·ª±a ch·ªçn t·ª´ UI v√†o env tr∆∞·ªõc khi kh·ªüi t·∫°o LLM
    os.environ["PROVIDER"] = provider_choice
    if provider_choice == "github":
        os.environ["GITHUB_MODELS_MODEL"] = ui_model
    else:
        os.environ["GOOGLE_MODEL"] = ui_model
    return LLMProvider.from_env()

@st.cache_data(show_spinner=True, ttl=60*60*12)  # cache 12h
def get_vendor_docs():
    urls = load_vendor_urls("sources.yaml")
    return fetch_vendor_docs(urls)

st.title("üì± Tr·ª£ l√Ω m√¥n SDN302 NodeJS (Gi·∫£ng vi√™n & H·ªçc vi√™n)")
st.caption("H·ªèi v·ªÅ kh√°i ni·ªám, best practices, lab/b√†i t·∫≠p (g·ª£i √Ω), quy ƒë·ªãnh m√¥n h·ªçc...")

# Session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "index" not in st.session_state:
    with st.spinner("ƒêang n·∫°p t√†i li·ªáu n·ªôi b·ªô v√† x√¢y d·ª±ng ch·ªâ m·ª•c..."):
        st.session_state.index = load_index()

# Kh·ªüi t·∫°o ho·∫∑c reload LLM khi ƒë·ªïi provider/model
need_reload_llm = (
    ("llm" not in st.session_state) or
    (st.session_state.get("llm_provider") != provider) or
    (st.session_state.get("llm_model") != model_name)
)
if need_reload_llm:
    with st.spinner("ƒêang kh·ªüi t·∫°o m√¥ h√¨nh..."):
        st.session_state.llm = load_llm(provider, model_name)
    st.session_state.llm_provider = provider
    st.session_state.llm_model = model_name

# Test ping
if test_clicked:
    try:
        if hasattr(st.session_state.llm, "ping"):
            with st.spinner("ƒêang ki·ªÉm tra k·∫øt n·ªëi LLM..."):
                result = st.session_state.llm.ping()
        else:
            result = "‚ùå ping() ch∆∞a ƒë∆∞·ª£c c√†i trong models.py ‚Äî vui l√≤ng c·∫≠p nh·∫≠t models.py."
    except Exception as e:
        ping_placeholder.error(f"‚ùå Ping exception: {e}")
    else:
        msg = str(result)
        (ping_placeholder.success if msg.startswith("‚úÖ") else ping_placeholder.error)(msg)

# Optional: upload t√†i li·ªáu b·ªï sung ngay trong app
uploaded_files = st.file_uploader(
    "T·∫£i th√™m t√†i li·ªáu (.md/.txt/.pdf) ƒë·ªÉ tƒÉng ch·∫•t l∆∞·ª£ng tr·∫£ l·ªùi",
    type=["md", "txt", "pdf"], accept_multiple_files=True
)
if uploaded_files:
    added = st.session_state.index.add_uploaded_files(uploaded_files)
    if added:
        st.success(f"ƒê√£ th√™m {added} t√†i li·ªáu t·∫£i l√™n v√†o ch·ªâ m·ª•c.")

# Vendor sync
vendor_docs = []
if use_vendor_docs:
    col1, col2 = st.columns([1,1])
    with col1:
        if st.button("üîÑ Sync ngu·ªìn vendor"):
            st.cache_data.clear()  # l√†m m·ªõi cache vendor docs
            st.rerun()
    with col2:
        st.caption("S·ª≠a URLs trong sources.yaml n·∫øu mu·ªën b·ªï sung/gi·∫£m b·ªõt ngu·ªìn.")
    with st.spinner("ƒêang t·∫£i ngu·ªìn vendor..."):
        vendor_docs = get_vendor_docs()
    if vendor_docs:
        st.info(f"ƒê√£ n·∫°p {len(vendor_docs)} trang vendor. S·∫Ω d√πng ƒë·ªÉ tr√≠ch d·∫´n khi RAG b·∫≠t.")

# K·∫øt h·ª£p ngu·ªìn theo l·ª±a ch·ªçn
st.session_state.index.reset_external_docs()
if use_vendor_docs and vendor_docs:
    st.session_state.index.add_external_docs(vendor_docs)
if not use_local_docs:
    st.session_state.index.disable_local_docs()

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat + tr√≠ch d·∫´n
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if "citations" in msg and msg["citations"]:
            with st.expander("Ngu·ªìn tr√≠ch d·∫´n"):
                for c in msg["citations"]:
                    st.write(f"- {c['source']} (score: {c['score']:.3f})")

question = st.chat_input("ƒê·∫∑t c√¢u h·ªèi v·ªÅ SDN302 NodeJS, b√†i t·∫≠p, lab, y√™u c·∫ßu m√¥n h·ªçc...")

def format_context(chunks):
    parts = []
    for i, ch in enumerate(chunks, 1):
        parts.append(f"[{i}] {ch['text']}\nSource: {ch['source']}")
    return "\n\n".join(parts)

if question:
    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang suy nghƒ©..."):
            retrieved = []
            context = ""
            if use_rag:
                retrieved = st.session_state.index.search(question, top_k=top_k)
                context = format_context(retrieved) if retrieved else ""
            try:
                answer = st.session_state.llm.generate_answer(
                    question=question,
                    context=context,
                    system_prompt=SYSTEM_PROMPT,
                    temperature=temperature
                )
                st.markdown(answer)
                citations = [{"source": r["source"], "score": r["score"]} for r in retrieved] if retrieved else []
                if citations:
                    with st.expander("Ngu·ªìn tr√≠ch d·∫´n"):
                        for c in citations:
                            st.write(f"- {c['source']} (score: {c['score']:.3f})")
            except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectTimeout) as e:
                st.error("LLM timeout. Th·ª≠ gi·∫£m 'S·ªë ƒëo·∫°n tr√≠ch d·∫´n (k)', ho·∫∑c h·ªèi ng·∫Øn h∆°n. B·∫°n c≈©ng c√≥ th·ªÉ tƒÉng timeout b·∫±ng c√°ch ƒë·∫∑t GITHUB_MODELS_TIMEOUT_READ trong Secrets (v√≠ d·ª• 180).")
                st.caption(f"Chi ti·∫øt: {e}")
                answer, citations = "Xin l·ªói, y√™u c·∫ßu m·∫•t qu√° l√¢u ƒë·ªÉ x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i.", []
            except Exception as e:
                st.error("C√≥ l·ªói khi g·ªçi m√¥ h√¨nh. Xem Logs ƒë·ªÉ bi·∫øt chi ti·∫øt.")
                st.caption(f"Chi ti·∫øt: {e}")
                answer, citations = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi l√∫c n√†y.", []
    st.session_state.messages.append({
        "role": "assistant",
        "content": answer,
        "citations": citations if use_rag else []
    })
